{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRooPBDZryc_",
        "colab_type": "text"
      },
      "source": [
        "Code to replicate sequence-to-sequence architecture, with Bahdanau attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbjOYeDG19w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://arxiv.org/pdf/1409.0473.pdf\n",
        "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "# https://blog.floydhub.com/attention-mechanism/\n",
        "# https://github.com/divyeshrajpura4114/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\n",
        "# https://colab.research.google.com/drive/1uFJBO1pgsiFwCGIJwZlhUzaJ2srDbtw-#scrollTo=KN8G-3YY8ADm"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C415mHjozxCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import re\n",
        "import spacy"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW7g1PkVTTTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrwzUsJuix3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ab887ed-2752-47ea-aa29-15f44927f93f"
      },
      "source": [
        "device"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Ww8oigzSHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "45d998f8-cf6f-4b77-d494-b94ec23b1f17"
      },
      "source": [
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p_g0Hcg1e5R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d3af9cb7-4327-42a4-9889-f7c633996b21"
      },
      "source": [
        "# https://stackoverflow.com/questions/56927602/unable-to-load-the-spacy-model-en-core-web-lg-on-google-colab\n",
        "# https://spacy.io/usage/models\n",
        "\n",
        "!pip install de_core_news_sm"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (47.3.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm) (1.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5tUOzUd4wJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import de_core_news_sm\n",
        "spacy_de = de_core_news_sm.load()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_eVHQdG4YlM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "a3f109af-40ed-420c-d483-71fe1936f95c"
      },
      "source": [
        "!pip install en_core_web_sm"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (47.3.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm) (1.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq5MgTZ45jUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import en_core_web_sm\n",
        "spacy_en = en_core_web_sm.load()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoGnCYb55nmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = re.compile('(<url>.*</url>)')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahkwwkBG1mkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84DpBWNP1sbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "DE = data.Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>')\n",
        "EN = data.Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWfdpGdh6rJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b2e91ed-c1e1-4ac4-bd64-25e1431b1f6c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/data/data/translation/'"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VHAElbx1uFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cbb121f9-dab3-478a-bc5d-a726654a64ad"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.Multi30k.splits(exts=('.de', '.en'), fields=(DE, EN))\n",
        "print('Loaded data...')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRR9469r7U7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "7d146939-a437-4a93-d51f-c164bf577892"
      },
      "source": [
        "print(train_data.fields)\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(vars(train_data[0]))\n",
        "print(vars(train_data[100]))\n",
        "print(vars(valid_data[100]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': <torchtext.data.field.Field object at 0x7fa7b6d244a8>, 'trg': <torchtext.data.field.Field object at 0x7fa7b6d24470>}\n",
            "29000\n",
            "1014\n",
            "{'src': ['Zwei', 'junge', 'weiÃŸe', 'MÃ¤nner', 'sind', 'im', 'Freien', 'in', 'der', 'NÃ¤he', 'vieler', 'BÃ¼sche', '.'], 'trg': ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
            "{'src': ['MÃ¤nnliches', 'Kleinkind', 'in', 'einem', 'roten', 'Hut', ',', 'das', 'sich', 'an', 'einem', 'GelÃ¤nder', 'festhÃ¤lt', '.'], 'trg': ['Toddler', 'boy', 'in', 'a', 'red', 'hat', 'holding', 'on', 'to', 'some', 'railings', '.']}\n",
            "{'src': ['Ein', 'Ã¤lterer', ',', 'Ã¼bergewichtiger', 'Mann', 'wendet', 'einen', 'Pfannkuchen', ',', 'wÃ¤hrend', 'er', 'FrÃ¼hstÃ¼ck', 'macht', '.'], 'trg': ['An', 'older', ',', 'overweight', 'man', 'flips', 'a', 'pancake', 'while', 'making', 'breakfast', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6XDPfZkB6p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set source and target language\n",
        "DE.build_vocab(train_data.src, min_freq=3)\n",
        "EN.build_vocab(train_data.trg, min_freq=3)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FwAMWsnCOs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "1dcdd577-e907-4536-99b6-3b1d26e19e61"
      },
      "source": [
        "train_iter, valid_iter = data.BucketIterator.splits((train_data, valid_data), batch_size=32, device=device)\n",
        "\n",
        "print(DE.vocab.freqs.most_common(10))\n",
        "print(len(DE.vocab))\n",
        "print(EN.vocab.freqs.most_common(10))\n",
        "print(len(EN.vocab))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 28821), ('Ein', 13904), ('einem', 13697), ('in', 11830), (',', 8938), ('und', 8925), ('mit', 8838), ('auf', 8686), ('Mann', 7805), ('einer', 6750)]\n",
            "5500\n",
            "[('a', 31707), ('.', 27623), ('A', 17458), ('in', 14847), ('the', 9923), ('on', 8019), ('is', 7524), ('and', 7378), ('man', 7359), ('of', 6871)]\n",
            "4727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4fR3w0HAkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, device):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        # input is of shape [1, batch_size]\n",
        "        # embedded is of shape [1, batch_size, embedding_size]\n",
        "\n",
        "        input = input.to(self.device)\n",
        "\n",
        "        embedded = self.embed(input)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        # output shape is [1, batch_size, hidden_dim]\n",
        "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kmQvH4PIjL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, n_dropout, device):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_dropout = n_dropout\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, self.embedding_size)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.n_dropout)\n",
        "\n",
        "        self.align = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.V = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "        self.appl_context = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, input, hidden, enc_outputs):\n",
        "        # input is of shape [batch_size]\n",
        "        # enc_outputs is of shape [sequence_len, batch_size, hidden_dim]\n",
        "        # hidden is of shape [1, batch_size, hiden_dim]\n",
        "\n",
        "        batch_size = hidden.size(1)\n",
        "\n",
        "        # Attention\n",
        "\n",
        "        hidden_context = hidden.clone().detach().requires_grad_(True)\n",
        "        hidden_context = hidden_context.repeat(enc_outputs.size(0), 1, 1)\n",
        "\n",
        "        # Hidden is expanded to sequence_len\n",
        "        # Hidden is now of shape [sequence_len, batch_size, hidden_dim]\n",
        "\n",
        "        attn_in = torch.cat((enc_outputs, hidden_context), dim=-1)\n",
        "        # combined enc_outputs and hidden:  [sequence_len, batch_size, hidden_dim * 2]\n",
        "\n",
        "        aligned = self.align(attn_in)\n",
        "        aligned = torch.tanh(aligned)\n",
        "        \n",
        "        aligned = aligned.permute(1, 2, 0)\n",
        "        #aligned = [batch size, hidden_dim, sequence_len]\n",
        "        \n",
        "        #v = [hidden_dim]\n",
        "        v = self.V.repeat(batch_size, 1).unsqueeze(1)\n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "\n",
        "        attention = torch.bmm(v, aligned).squeeze(1)\n",
        "        #attention = [batch size, src len]\n",
        "\n",
        "        alphas = F.softmax(attention, dim=1)\n",
        "        #alphas = [batch_size, sequence_len]\n",
        "\n",
        "        alphas = alphas.unsqueeze(1)\n",
        "        #alphas =  [batch_size, 1, sequence_len]\n",
        "\n",
        "        # enc_outputs is of shape [sequence_len, batch_size, hidden_dim]\n",
        "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
        "        #enc_outputs = [batch_size, sequence_len, hidden_dim]\n",
        "\n",
        "        context = torch.bmm(alphas, enc_outputs)\n",
        "        #weighted = [batch_size, 1, hidden_dim]\n",
        "        \n",
        "        context = context.permute(1, 0, 2)\n",
        "        #context: [1, batch_size, hidden_dim]\n",
        "\n",
        "        # Decoder\n",
        "\n",
        "        batch_size = hidden.size(1)\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # now, input shape is [1, batch_size]\n",
        "\n",
        "        input = input.to(self.device)\n",
        "\n",
        "        embedded = self.embed(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded is of shape [1, batch_size, embedding_dim]\n",
        "\n",
        "        output = torch.cat((embedded, context), dim=-1)\n",
        "        #output:  [1, batch_size, hidden_dim * 2]\n",
        "\n",
        "        output = self.appl_context(output)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        #output:  [1, batch_size, hidden_dim]\n",
        "\n",
        "        output = self.out(output.squeeze(0))\n",
        "        #output:  [1, batch_size, vocab_size]\n",
        "\n",
        "        output = self.logsoftmax(output)\n",
        "\n",
        "        return output, hidden, alphas\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lPdNYu4wbAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(enc)\n",
        "#print(dec)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYqrDNmxV7Nq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "53692bb1-f207-4802-b52f-4fc9ee9999f7"
      },
      "source": [
        "vocab_size_src = len(DE.vocab)\n",
        "vocab_size_trg = len(EN.vocab)\n",
        "\n",
        "embedding_size = 300\n",
        "hidden_size = 128\n",
        "n_dropout = 0.1\n",
        "\n",
        "enc = EncoderRNN(vocab_size_src, embedding_size, hidden_size, device)\n",
        "dec = AttentionDecoderRNN(vocab_size_trg, embedding_size, hidden_size, n_dropout, device)\n",
        "\n",
        "enc_optim = optim.Adam(enc.parameters())\n",
        "dec_optim = optim.Adam(dec.parameters())\n",
        "    \n",
        "pad_idx = EN.vocab.stoi['<pad>']\n",
        "print('Pad index: ', pad_idx)\n",
        "criterion = nn.NLLLoss(ignore_index=pad_idx)\n",
        "\n",
        "epochs = 20\n",
        "clip = 10\n",
        "\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        hidden = enc.init_hidden(batch.src.size(1))\n",
        "\n",
        "        max_len_enc = batch.src.size(0)\n",
        "\n",
        "        enc_outputs = torch.zeros(max_len_enc, batch.src.size(1), hidden_size, device=device)\n",
        "        for i in range(max_len_enc):\n",
        "            out, hidden = enc(batch.src[i].unsqueeze(0), hidden)\n",
        "            enc_outputs[i] = out\n",
        "\n",
        "        # store outputs\n",
        "        max_len_dec = batch.trg.size(0)\n",
        "\n",
        "        outputs = torch.zeros(max_len_dec, batch.trg.size(1), vocab_size_trg, device=device)\n",
        "        input = batch.trg[0]\n",
        "        \n",
        "        for i in range(1, max_len_dec):\n",
        "            output, hidden, _ = dec(input, hidden, enc_outputs)\n",
        "            outputs[i] = output\n",
        "            input = batch.trg[i]\n",
        "\n",
        "        enc_optim.zero_grad()\n",
        "        dec_optim.zero_grad()\n",
        "\n",
        "        target = torch.tensor(batch.trg[1:], device=device)\n",
        "        loss = criterion(outputs[1:].view(-1, outputs.shape[2]), target.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(enc.parameters(), clip)\n",
        "        nn.utils.clip_grad_norm_(dec.parameters(), clip)\n",
        "\n",
        "        enc_optim.step()\n",
        "        dec_optim.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        print('\\rEpoch {} : Loss {:.3f}'.format(epoch, epoch_loss / len(batch)), end=\"\")\n",
        "\n",
        "    print('\\rEpoch {} : Loss {:.3f}'.format(epoch, epoch_loss / len(train_iter)))\n",
        "\n",
        "    epoch_losses.append(epoch_loss / len(train_iter))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad index:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : Loss 4.094\n",
            "Epoch 1 : Loss 3.193\n",
            "Epoch 2 : Loss 2.749\n",
            "Epoch 3 : Loss 2.439\n",
            "Epoch 4 : Loss 2.205\n",
            "Epoch 5 : Loss 2.022\n",
            "Epoch 6 : Loss 1.873\n",
            "Epoch 7 : Loss 1.751\n",
            "Epoch 8 : Loss 1.647\n",
            "Epoch 9 : Loss 1.558\n",
            "Epoch 10 : Loss 1.481\n",
            "Epoch 11 : Loss 1.412\n",
            "Epoch 12 : Loss 1.350\n",
            "Epoch 13 : Loss 1.295\n",
            "Epoch 14 : Loss 1.244\n",
            "Epoch 15 : Loss 1.200\n",
            "Epoch 16 : Loss 1.160\n",
            "Epoch 17 : Loss 1.118\n",
            "Epoch 18 : Loss 1.084\n",
            "Epoch 19 : Loss 1.052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJzP-aiMpj84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "ca6cc048-9bfc-455a-f700-2f1303470ffa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epoch_losses)\n",
        "plt.title(\"Training loss\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiXYRMKkqiyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_attention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ6RHiXNPeEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "5f4cfad7-e112-4eed-a110-309fc826cc75"
      },
      "source": [
        "sos_idx = EN.vocab.stoi['<sos>']\n",
        "eos_idx = EN.vocab.stoi['<eos>']\n",
        "\n",
        "batch = next(iter(valid_iter))\n",
        "\n",
        "for i in range(5):\n",
        "    with torch.no_grad():\n",
        "        encoder_input = batch.src[:, i].unsqueeze(1)\n",
        "        hidden = enc.init_hidden(1)\n",
        "\n",
        "        encoded = []\n",
        "        max_len_enc = batch.src.size(0)\n",
        "        enc_outputs = torch.zeros(max_len_enc, 1, hidden_size, device=device)\n",
        "        for ip in encoder_input:\n",
        "            encoded.append(DE.vocab.itos[ip.item()])\n",
        "            out, hidden = enc(ip.unsqueeze(0), hidden)\n",
        "            enc_outputs[i] = out\n",
        "\n",
        "        max_len_dec = batch.trg.size(0)\n",
        "        decoder_input = torch.tensor([sos_idx], device=device)\n",
        "        decoder_attentions = torch.zeros(max_len_dec*2, max_len_enc)\n",
        "\n",
        "        decoded = []\n",
        "        decoded.append(EN.vocab.itos[sos_idx])\n",
        "        true_output = batch.trg[:, i].unsqueeze(1)\n",
        "        true = [EN.vocab.itos[w] for w in true_output]\n",
        "\n",
        "        k = 0\n",
        "        while True:\n",
        "            output, hidden, alphas = dec(decoder_input, hidden, enc_outputs)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi.squeeze(1)\n",
        "            decoder_input = topi.detach()\n",
        "            \n",
        "            decoder_attentions[k] = alphas\n",
        "            k += 1\n",
        "            \n",
        "            decoded.append(EN.vocab.itos[decoder_input.item()])\n",
        "            if decoder_input.item() == eos_idx:\n",
        "                break\n",
        "\n",
        "        print(encoded)\n",
        "        print(decoded)\n",
        "        print(true)\n",
        "        print()\n",
        "\n",
        "        #show_attention(encoded, decoded, decoder_attentions)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<sos>', 'Ein', 'Kind', 'sitzt', 'auf', 'einer', '<unk>', '.', '<eos>']\n",
            "['<sos>', 'A', 'high', '-', '<unk>', 'is', 'riding', 'a', 'beige', '-', '<unk>', 'in', 'a', 'forest', '.', '<eos>']\n",
            "['<sos>', 'A', 'child', 'sitting', 'on', 'a', 'rock', 'formation', '.', '<eos>', '<pad>', '<pad>', '<pad>']\n",
            "\n",
            "['<sos>', 'Zwei', 'Frauen', 'lÃ¤cheln', 'bei', 'einer', 'Veranstaltung', '.', '<eos>']\n",
            "['<sos>', 'Two', 'women', 'in', 'a', 'military', 'camouflage', 'shirt', 'are', 'flipping', 'hand', 'on', 'a', 'shoulder', '.', '<eos>']\n",
            "['<sos>', 'Two', 'women', 'are', 'smiling', 'at', 'an', 'event', '.', '<eos>', '<pad>', '<pad>', '<pad>']\n",
            "\n",
            "['<sos>', 'Zwei', 'Pudel', 'rennen', 'durch', 'den', 'Schnee', '.', '<eos>']\n",
            "['<sos>', 'Two', 'women', 'in', 'red', 'and', 'gray', 'snow', 'are', 'wearing', 'a', 'life', 'jacket', '.', '<eos>']\n",
            "['<sos>', 'Two', 'poodles', 'are', 'running', 'through', 'the', 'snow', '.', '<eos>', '<pad>', '<pad>', '<pad>']\n",
            "\n",
            "['<sos>', '<unk>', '<unk>', 'tagsÃ¼ber', 'ihre', '<unk>', '.', '<eos>', '<pad>']\n",
            "['<sos>', '<unk>', ',', 'a', 'couple', 'of', 'blue', 'and', 'white', 'lab', 'is', 'taking', 'place', 'in', 'the', 'woods', '.', '<eos>']\n",
            "['<sos>', '<unk>', 'are', 'performing', 'their', '<unk>', 'during', 'the', 'day', '.', '<eos>', '<pad>', '<pad>']\n",
            "\n",
            "['<sos>', 'Drei', 'Hunde', 'spielen', 'im', 'Wasser', '.', '<eos>', '<pad>']\n",
            "['<sos>', 'Three', 'people', 'in', 'red', 'and', 'purple', 'hiking', 'gear', '.', '<eos>']\n",
            "['<sos>', 'Three', 'dogs', 'are', 'playing', 'in', 'the', 'water', '.', '<eos>', '<pad>', '<pad>', '<pad>']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n7ClZmtxwSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "db32a1b4-d945-4aff-c16e-78ecfad38af2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_src = [['<sos>', 'Ein', 'Kind', 'sitzt', 'auf', 'einer', '<unk>', '.', '<eos>'], \n",
        "             ['<sos>', 'Zwei', 'Frauen', 'lÃ¤cheln', 'bei', 'einer', 'Veranstaltung', '.', '<eos>'],\n",
        "             ['<sos>', 'Zwei', 'Pudel', 'rennen', 'durch', 'den', 'Schnee', '.', '<eos>'],\n",
        "             ['<sos>', '<unk>', '<unk>', 'tagsÃ¼ber', 'ihre', '<unk>', '.', '<eos>', '<pad>'],\n",
        "             ['<sos>', 'Drei', 'Hunde', 'spielen', 'im', 'Wasser', '.', '<eos>', '<pad>']]\n",
        "batch_src = np.array(batch_src)\n",
        "\n",
        "batch_src_tensor = torch.zeros(batch_src.shape[0], batch_src.shape[1], dtype=torch.long)\n",
        "\n",
        "for n in range(batch_src.shape[0]):\n",
        "    for m in range(batch_src.shape[1]):\n",
        "        batch_src_tensor[n, m] = DE.vocab.stoi[batch_src[n, m]]\n",
        "\n",
        "batch_src_tensor = batch_src_tensor.T\n",
        "print(batch_src_tensor)\n",
        "\n",
        "for i in range(5):\n",
        "    with torch.no_grad():\n",
        "        encoder_input = batch_src_tensor[:, i].unsqueeze(1)\n",
        "        hidden = enc.init_hidden(1)\n",
        "\n",
        "        encoded = []\n",
        "        max_len_enc = batch_src_tensor.size(0)\n",
        "        enc_outputs = torch.zeros(max_len_enc, 1, hidden_size, device=device)\n",
        "        for ip in encoder_input:\n",
        "            encoded.append(DE.vocab.itos[ip.item()])\n",
        "            out, hidden = enc(ip.unsqueeze(0), hidden)\n",
        "            enc_outputs[i] = out\n",
        "\n",
        "        max_len_dec = batch_src_tensor.size(0)\n",
        "        decoder_input = torch.tensor([sos_idx], device=device)\n",
        "        decoder_attentions = torch.zeros(max_len_dec*2, max_len_enc)\n",
        "\n",
        "        decoded = []\n",
        "        decoded.append(EN.vocab.itos[sos_idx])\n",
        "\n",
        "        k = 0\n",
        "        while True:\n",
        "            output, hidden, alphas = dec(decoder_input, hidden, enc_outputs)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi.squeeze(1)\n",
        "            decoder_input = topi.detach()\n",
        "            \n",
        "            decoder_attentions[k] = alphas\n",
        "            k += 1\n",
        "            \n",
        "            decoded.append(EN.vocab.itos[decoder_input.item()])\n",
        "            if decoder_input.item() == eos_idx:\n",
        "                break\n",
        "\n",
        "        print(encoded)\n",
        "        print(decoded)\n",
        "        print()\n",
        "\n",
        "        #show_attention(encoded, decoded, decoder_attentions)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   2,    2,    2,    2,    2],\n",
            "        [   5,   21,   21,    0,   60],\n",
            "        [  50,   46, 1751,    0,  123],\n",
            "        [  32,  328,  236, 2274,   59],\n",
            "        [  11,   63,   58,  128,   22],\n",
            "        [  13,   13,   34,    0,   65],\n",
            "        [   0,  521,  125,    4,    4],\n",
            "        [   4,    4,    4,    3,    3],\n",
            "        [   3,    3,    3,    1,    1]])\n",
            "['<sos>', 'Ein', 'Kind', 'sitzt', 'auf', 'einer', '<unk>', '.', '<eos>']\n",
            "['<sos>', 'A', 'high', 'jumper', 'is', 'doing', 'a', '<unk>', 'trick', 'on', 'a', 'red', 'barrier', '.', '<eos>']\n",
            "\n",
            "['<sos>', 'Zwei', 'Frauen', 'lÃ¤cheln', 'bei', 'einer', 'Veranstaltung', '.', '<eos>']\n",
            "['<sos>', 'Two', 'women', 'in', 'a', 'military', 'camouflage', 'shirt', '<unk>', 'on', 'a', 'brick', 'wall', '.', '<eos>']\n",
            "\n",
            "['<sos>', 'Zwei', 'Pudel', 'rennen', 'durch', 'den', 'Schnee', '.', '<eos>']\n",
            "['<sos>', 'Two', 'women', 'in', 'red', 'and', 'gray', 'snow', 'covered', 'in', 'a', 'snow', '.', '<eos>']\n",
            "\n",
            "['<sos>', '<unk>', '<unk>', 'tagsÃ¼ber', 'ihre', '<unk>', '.', '<eos>', '<pad>']\n",
            "['<sos>', 'A', '<unk>', 'of', 'young', 'Asian', 'people', 'in', 'a', 'gold', 'session', ',', 'is', 'surrounded', 'by', 'trees', '.', '<eos>']\n",
            "\n",
            "['<sos>', 'Drei', 'Hunde', 'spielen', 'im', 'Wasser', '.', '<eos>', '<pad>']\n",
            "['<sos>', 'Three', 'women', 'in', 'summer', 'clothes', 'are', 'playing', 'soccer', 'game', '.', '<eos>']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}