{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pytorch4]",
      "language": "python",
      "name": "conda-env-pytorch4-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "vail.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E_XaVsmyQs3L"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCzg9ePxgUl4"
      },
      "source": [
        "Modified from https://github.com/higgsfield/RL-Adventure-2 using SB3\n",
        "\n",
        "References:\n",
        "\n",
        "https://colab.research.google.com/github/araffin/rl-handson-rlvs21/blob/main/rlvs_hands_on_sb3.ipynb\n",
        "\n",
        "https://github.com/reinforcement-learning-kr/lets-do-irl\n",
        "\n",
        "Highly experimental code!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fQeVtEYhIoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4bdeb1-6a11-4e9b-de7b-aeb6d3a8d25f"
      },
      "source": [
        "!apt install swig\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (1,518 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines3[extra]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/6ae6e774ac6cf8f5eeca1c30b9125231db901b75f72da7d81e939f293f69/stable_baselines3-1.0-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.8.1+cu101)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.2.6)\n",
            "Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.4.1)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (56.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.0.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.30.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.12.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9hfNbmUfgEl"
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QAfYQMjfgEx"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kEb88vUqiZl"
      },
      "source": [
        "from typing import Any, Dict, Optional, Type, Union, List, Callable\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "from stable_baselines3.common import logger\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, BasePolicy\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.env_util import make_vec_env as mk_env\n",
        "from stable_baselines3.common.utils import configure_logger\n",
        "from stable_baselines3 import PPO\n",
        "from gym import spaces"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxUij6P5fgEz"
      },
      "source": [
        "<h2>Use CUDA</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGiG9ItjfgE1"
      },
      "source": [
        "use_cuda = th.cuda.is_available()\n",
        "device   = th.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRqNQjdifgE2"
      },
      "source": [
        "<h2>Create Environments</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcHThrDWfgE2"
      },
      "source": [
        "num_envs = 8\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "envs = mk_env(env_id, n_envs=num_envs)\n",
        "eval_env = gym.make(env_id)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iZ5yVq5fgE6"
      },
      "source": [
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn9c6qq4fgE_"
      },
      "source": [
        "<h2>Loading expert trajectories from №3 notebook</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvQ-KrCDfgFA",
        "outputId": "03daebe4-3c4e-47c8-c3cf-fd19ed98c259"
      },
      "source": [
        "try:\n",
        "    expert_traj = np.load(\"expert_traj.npy\")\n",
        "    expert_traj = th.from_numpy(expert_traj)\n",
        "except:\n",
        "    print(\"Train, generate and save expert trajectories in notebook №3\")\n",
        "    expert_model = PPO('MlpPolicy', envs, seed=42)\n",
        "    expert_model.learn(total_timesteps=25000)\n",
        "    n_eval = 10000\n",
        "\n",
        "    expert_traj = []\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "\n",
        "    for _ in range(n_eval):\n",
        "        action, states = expert_model.predict(obs)\n",
        "        act = np.zeros(envs.action_space.n)\n",
        "        act[action] = 1\n",
        "        expert_traj.append(np.concatenate((obs, act)))\n",
        "        obs, _, done, _ = eval_env.step(action)\n",
        "        if done:\n",
        "            obs = eval_env.reset()\n",
        "\n",
        "    expert_traj = np.array(expert_traj, dtype=np.float32)\n",
        "    th.save(expert_traj, 'expert_traj.npy')\n",
        "    expert_traj = th.from_numpy(expert_traj)\n",
        "\n",
        "print(expert_traj.shape)\n",
        "print(expert_traj[:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train, generate and save expert trajectories in notebook №3\n",
            "torch.Size([10000, 6])\n",
            "tensor([[ 0.0099, -0.0051,  0.0384, -0.0310,  1.0000,  0.0000],\n",
            "        [ 0.0098, -0.2007,  0.0378,  0.2736,  0.0000,  1.0000],\n",
            "        [ 0.0058, -0.0062,  0.0433, -0.0069,  1.0000,  0.0000],\n",
            "        [ 0.0056, -0.2019,  0.0431,  0.2991,  1.0000,  0.0000],\n",
            "        [ 0.0016, -0.3976,  0.0491,  0.6051,  0.0000,  1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfCcJR5mfgFA"
      },
      "source": [
        "<h1>Generative Adversarial Imitation Learning</h1>\n",
        "<h2><a href=\"https://arxiv.org/abs/1606.03476\">Arxiv</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rc1_mQRfgFB"
      },
      "source": [
        "class VDB(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size, latent_size, device):\n",
        "        super(VDB, self).__init__()\n",
        "        \n",
        "        self.vae_encoder_input = nn.Linear(num_inputs, hidden_size)\n",
        "        self.vae_encoder_mu = nn.Linear(hidden_size, latent_size)\n",
        "        self.vae_encoder_logvar = nn.Linear(hidden_size, latent_size)\n",
        "        \n",
        "        self.discrim_input = nn.Linear(latent_size, hidden_size)\n",
        "        self.discrim_output = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "        self.discrim_output.weight.data.mul_(0.1)\n",
        "        self.discrim_output.bias.data.mul_(0.0)\n",
        "        \n",
        "        self.to(device)\n",
        "\n",
        "    def vae_encoder(self, x):\n",
        "        h = th.tanh(self.vae_encoder_input(x))\n",
        "        return self.vae_encoder_mu(h), self.vae_encoder_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = th.exp(logvar/2)\n",
        "        eps = th.randn_like(std)\n",
        "        return mu + std * eps\n",
        "\n",
        "    def discriminator(self, z):\n",
        "        z = th.tanh(self.discrim_input(z))\n",
        "        prob = th.sigmoid(self.discrim_output(z))\n",
        "        return prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.vae_encoder(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        prob = self.discriminator(z)\n",
        "        return prob, mu, logvar"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU_uvMkNCQeG"
      },
      "source": [
        "class VecCustomReward(VecEnvWrapper):\n",
        "    def __init__(\n",
        "        self,\n",
        "        venv: VecEnv,\n",
        "        vdb: VDB\n",
        "    ):\n",
        "        VecEnvWrapper.__init__(self, venv)\n",
        "\n",
        "        self.vdb = vdb\n",
        "        \n",
        "    def reset(self):\n",
        "        return self.venv.reset()\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rewards, dones, infos = self.venv.step_wait()\n",
        "\n",
        "        rewards = self._update_reward(obs, self.venv.actions, self.action_space.n)\n",
        "\n",
        "        return obs, rewards, dones, infos\n",
        "\n",
        "    def _update_reward(self, states, actions, n_actions):\n",
        "        \"\"\"Update reward using discriminator.\"\"\"\n",
        "        acts = th.zeros((actions.shape[0], n_actions), dtype=th.float32)\n",
        "        action_idx = th.from_numpy(np.array(actions)).to(th.int64).view(-1, 1)\n",
        "        actions = acts.scatter(1, action_idx, 1)\n",
        "        state_action = th.FloatTensor(np.concatenate([states, actions], 1))\n",
        "        return -np.log(self.vdb(state_action)[0].view(-1).cpu().data.numpy())\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXh7skIjyqYP"
      },
      "source": [
        "def kl_divergence(mu, logvar):\n",
        "    kl_div = 0.5 * th.sum(mu.pow(2) + logvar.exp() - logvar - 1, dim=1)\n",
        "    return kl_div"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQx_tgwtwNfL"
      },
      "source": [
        "class VAIL(BaseAlgorithm):\n",
        "    def __init__(self,\n",
        "                 policy,\n",
        "                 env: Union[GymEnv, str],\n",
        "                 vdb: VDB,\n",
        "                 expert_traj: th.tensor,\n",
        "                 policy_base: Type[BasePolicy] = None,\n",
        "                 learning_rate: Union[float, Schedule] = 3e-4,\n",
        "                 beta: float = 0,\n",
        "                 alpha_beta: float = 1e-4,\n",
        "                 i_c: float = 0.5,\n",
        "                 policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "                 tensorboard_log: Optional[str] = None,\n",
        "                 verbose: int = 0,\n",
        "                 support_multi_env: bool = True,\n",
        "                 device: Union[th.device, str] = \"auto\",\n",
        "                 create_eval_env: bool = False,\n",
        "                 seed: Optional[int] = None,\n",
        "                 _init_setup_model: bool = True,\n",
        "                 ):\n",
        "        super(VAIL, self).__init__(policy=policy, env=env, learning_rate=learning_rate,\n",
        "                                   policy_base=policy_base, support_multi_env=support_multi_env,\n",
        "                                   policy_kwargs=policy_kwargs, tensorboard_log=tensorboard_log,\n",
        "                                   verbose=verbose, device=device,\n",
        "                                   create_eval_env=create_eval_env, seed=seed,\n",
        "                                   supported_action_spaces=(\n",
        "                                                    spaces.Box,\n",
        "                                                    spaces.Discrete,\n",
        "                                                    spaces.MultiDiscrete,\n",
        "                                                    spaces.MultiBinary,\n",
        "                                                )\n",
        "                                   )\n",
        "        self.vdb = vdb\n",
        "        self.expert_traj = expert_traj\n",
        "        self.policy = policy(**policy_kwargs)\n",
        "        self.beta = beta\n",
        "        self.alpha_beta = alpha_beta\n",
        "        self.i_c = i_c\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def get_generator_batch(self, num_samples):\n",
        "        '''shuffle, and get a batch of num_samples of state, action tensor from\n",
        "           rollout buffer\n",
        "        '''\n",
        "        return self.policy.rollout_buffer.sample(num_samples)\n",
        "\n",
        "    def train_vdb(self, num_samples):\n",
        "        expert_state_action = self.expert_traj[np.random.randint(0,\n",
        "                                                                 self.expert_traj.shape[0],\n",
        "                                                                 num_samples),\n",
        "                                                :]\n",
        "        expert_state_action = th.FloatTensor(expert_state_action).to(device)\n",
        "\n",
        "        num_samples = expert_state_action.shape[0]\n",
        "\n",
        "        state_action = self.get_generator_batch(num_samples)\n",
        "\n",
        "        acts = th.zeros((num_samples, self.action_space.n), dtype=th.float32)\n",
        "        action_idx = state_action.actions.to(th.int64)\n",
        "        actions = acts.scatter(1, action_idx, 1)\n",
        "        state_action = th.cat([state_action.observations, actions], 1)\n",
        "\n",
        "        fake, l_mu, l_logvar = self.vdb(state_action)\n",
        "        real, e_mu, e_logvar = self.vdb(expert_state_action)\n",
        "\n",
        "        l_kld = kl_divergence(l_mu, l_logvar)\n",
        "        l_kld = l_kld.mean()\n",
        "        \n",
        "        e_kld = kl_divergence(e_mu, e_logvar)\n",
        "        e_kld = e_kld.mean()\n",
        "        \n",
        "        kld = 0.5 * (l_kld + e_kld)\n",
        "        bottleneck_loss = kld - self.i_c\n",
        "\n",
        "        beta = max(0, self.beta + self.alpha_beta * bottleneck_loss)\n",
        "\n",
        "        self.optimizer_vdb.zero_grad()\n",
        "        vdb_losses = self.discrim_criterion(fake, th.ones((num_samples, 1))) + \\\n",
        "                       self.discrim_criterion(real, th.zeros((num_samples, 1))) +\\\n",
        "                       beta * bottleneck_loss\n",
        "        vdb_losses.backward()\n",
        "        self.optimizer_vdb.step()\n",
        "\n",
        "        return vdb_losses\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        self._setup_lr_schedule()\n",
        "        self.set_random_seed(self.seed)\n",
        "\n",
        "        self.optimizer_vdb = optim.Adam(self.vdb.parameters(), lr=self.learning_rate)\n",
        "        self.discrim_criterion = nn.BCELoss()\n",
        " \n",
        "    def learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 1,\n",
        "        eval_env: Optional[GymEnv] = None,\n",
        "        eval_freq: int = -1,\n",
        "        n_eval_episodes: int = 5,\n",
        "        tb_log_name: str = \"VAILAlgorithm\",\n",
        "        eval_log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "        ) -> \"VAIL\":\n",
        "\n",
        "        iteration = 1  #since we have one ppo.train() outside the loop\n",
        "\n",
        "        total_timesteps, callback = self._setup_learn(\n",
        "            total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, eval_log_path,\n",
        "            reset_num_timesteps, tb_log_name\n",
        "        )\n",
        "\n",
        "        callback.on_training_start(locals(), globals())\n",
        "\n",
        "        # need to fill the rollout buffer\n",
        "        self.policy.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "        while self.num_timesteps < total_timesteps:\n",
        "\n",
        "            iteration += 1\n",
        "            self.num_timesteps += 1\n",
        "\n",
        "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
        "\n",
        "            dloss = self.train_vdb(self.policy.n_steps * self.policy.n_envs)\n",
        "\n",
        "            # Display training infos\n",
        "            fps = int(self.num_timesteps / (time.time() - self.start_time))\n",
        "            logger.record(\"vail/time/fps\", fps)\n",
        "            logger.record(\"vail/time/iterations\", iteration)\n",
        "            logger.record(\"vail/time/time_elapsed\", int(time.time() - self.start_time), exclude=\"tensorboard\")\n",
        "            logger.record(\"vail/time/number_timesteps\", self.num_timesteps)\n",
        "            logger.record(\"vail/time/ppo_timesteps\", self.policy.num_timesteps)\n",
        "            logger.record_mean(\"vail/train/vdb_loss\", dloss.item())\n",
        "            logger.dump(step=self.num_timesteps)\n",
        "\n",
        "            if iteration % 3 == 0:\n",
        "                self.policy.learn(total_timesteps=total_timesteps, reset_num_timesteps=False)\n",
        "\n",
        "        callback.on_training_end()\n",
        "\n",
        "        return self\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFDVurDPg5n"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_XaVsmyQs3L"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_VWz47EOZEk",
        "outputId": "37c2cb0c-dc80-4126-c8bf-f892a1fd3d90"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 1s (1,226 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 161497 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ACKdK3Oh06"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRIdda_fPYJY"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9A8txg0Pbaa"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5g0Eu18yC8r"
      },
      "source": [
        "# Testing VAIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a4-rMRxDzuo"
      },
      "source": [
        "%tensorboard --logdir=./results/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "rH61dv3zNQTJ",
        "outputId": "098ee24d-4cc9-4776-e3eb-94d4c5f08c6f"
      },
      "source": [
        "# https://github.com/araffin/rl-baselines-zoo/blob/master/hyperparams/ppo2.yml\n",
        "'''\n",
        "# Tuned\n",
        "CartPole-v1:\n",
        "  n_envs: 8\n",
        "  n_timesteps: !!float 1e5\n",
        "  policy: 'MlpPolicy'\n",
        "  n_steps: 32\n",
        "  nminibatches: 1\n",
        "  lam: 0.8\n",
        "  gamma: 0.98\n",
        "  noptepochs: 20\n",
        "  ent_coef: 0.0\n",
        "  learning_rate: lin_0.001\n",
        "  cliprange: lin_0.2\n",
        "'''"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Tuned\\nCartPole-v1:\\n  n_envs: 8\\n  n_timesteps: !!float 1e5\\n  policy: 'MlpPolicy'\\n  n_steps: 32\\n  nminibatches: 1\\n  lam: 0.8\\n  gamma: 0.98\\n  noptepochs: 20\\n  ent_coef: 0.0\\n  learning_rate: lin_0.001\\n  cliprange: lin_0.2\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mto0xg_-YMuk"
      },
      "source": [
        "num_envs = 8\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "num_inputs  = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.n\n",
        "hidden_size = 128\n",
        "latent_size = 4\n",
        "\n",
        "vdb = VDB(num_inputs + num_outputs, hidden_size, latent_size, device)\n",
        "\n",
        "envs = mk_env(env_id, n_envs=num_envs)\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "envs = VecCustomReward(envs, vdb=vdb)\n",
        "\n",
        "# verbose must be 1 for both VAIL and Policy algorithm to print on console\n",
        "vail = VAIL(PPO, envs, vdb, expert_traj=expert_traj, tensorboard_log=\"./results\",\n",
        "            verbose=1,\n",
        "            policy_kwargs={'policy': 'MlpPolicy', 'env': envs, 'verbose': 1,\n",
        "                           'n_steps': 32})\n",
        "vail.learn(total_timesteps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB9mN93Ky6tJ",
        "outputId": "0978cf8b-ed2d-4da3-b05d-32f099fc00ad"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(vail, eval_env, n_eval_episodes=100)\n",
        "print(f\"Mean reward: {mean_reward:.2f}, Std Reward: {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 282.79, Std Reward: 58.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rq2jiBxzbiV"
      },
      "source": [
        "record_video('CartPole-v1', vail, video_length=100, prefix='vail-cartpole')\n",
        "show_videos('videos', prefix='vail-cartpole')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewjQsxb7gxjf"
      },
      "source": [
        "# Testing against Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOhSildK1NFk"
      },
      "source": [
        "%tensorboard --logdir=./ppo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMF2XMT07ET"
      },
      "source": [
        "num_envs = 8\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "envs_ppo = mk_env(env_id, n_envs=num_envs)\n",
        "eval_env_ppo = gym.make(env_id)\n",
        "\n",
        "model_test = PPO('MlpPolicy', envs_ppo, verbose=1, tensorboard_log=\"./ppo\", n_steps=32)\n",
        "model_test.learn(total_timesteps=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAvTacxdtwbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e4a55c-02a3-48d8-9c00-54cf6aa39f79"
      },
      "source": [
        "# Evaluate the agent\n",
        "# NOTE: If you use wrappers with your environment that modify rewards,\n",
        "#       this will be reflected here. To evaluate with original rewards,\n",
        "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
        "mean_reward, std_reward = evaluate_policy(model_test, eval_env_ppo, n_eval_episodes=100)\n",
        "print(f\"Mean reward: {mean_reward:.2f}, Std Reward: {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 219.38, Std Reward: 68.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMbSRIsFPdMV",
        "outputId": "27b0abb9-8c3b-4ab4-8a6f-145e16f2ea17"
      },
      "source": [
        "record_video('CartPole-v1', model_test, video_length=100, prefix='ppo-cartpole')\n",
        "show_videos('videos', prefix='ppo-cartpole')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving video to  /content/videos/ppo-cartpole-step-0-to-step-100.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}